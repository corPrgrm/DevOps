文章链接：《Kubernetes Handbook——Kubernetes 中文指南/云原生应用架构实践手册》：https://jimmysong.io/kubernetes-handbook/
	 《命令操作手册》						    https://www.kubernetes.org.cn/docs


云原生：

					使用轻量级的容器打包
					使用最合适的语言和框架开发
					以松耦合的微服务方式设计
					以API为中心的交互和协作
					无状态和有状态服务在架构上界限清晰
					不依赖于底层操作系统和服务器
					部署在自服务、弹性的云基础设施上
					通过敏捷的DevOps流程管理
					自动化能力
					通过定义和策略驱动的资源分配



首先说一下Docker、Kubernetes、Helm都是什么，和三者的关系
						- Docker是用来打包程序的配置文件和程序本身的容器（container）技术。docker生成的container，可以在任何环境（无论你是windows电脑，还是linux）里运行，不需要再做任何配置。
						- kubernetes用来部署docker的containers。
						- helm是kubernetes的包管理软件。
						云原生的代表技术包括 容器、服务网格、微服务、不可变基础设施 和 声明式 API。\
							面向分布式设计（Distribution）：容器、微服务、API 驱动的开发；
							面向配置设计（Configuration）：一个镜像，多个环境配置；
							面向韧性设计（Resistancy）：故障容忍和自愈；
							面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应；
							面向交付设计（Delivery）：自动拉起，缩短交付时间；
							面向性能设计（Performance）：响应式，并发和资源高效利用；
							面向自动化设计（Automation）：自动化的 DevOps；
							面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪；
							面向安全性设计（Security）：安全端点、API Gateway、端到端加密；
							云原生不是微服务或基础设施即代码。微服务意味着更快的开发周期和更小的独特功能，但是单片应用程序可以具有相同的功能，使其能够通过软件有效管理，并且还可以从云原生基础设施中受益。

					微服务场景：
						 存在性能问题的服务，微服务化后可以扩展
						 变动多的服务
						 可以单独提供对外能力的服务

					解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。	 

					更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。

					与容器相关的一个重要概念是微服务。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，**** 微服务与容器化是相辅相成的。***


					声明式编程和命令式编程有什么区别？ https://www.zhihu.com/question/22285830
					最重要的是你需要认识到 Kubernetes 利用了 “期望状态” 原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。


					服务网格 (Service Mesh) 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 Kubernetes 次世代的云原生应用 的重要组成部分。
					Istio Handbook——Istio 服务网格进阶实战。 *** 动态切换服务进行测试。。。动态跟踪服务。。展示


					Kubernetes通过声明式配置，真正让开发人员能够理解应用的状态，并通过同一份配置可以立马启动一个一模一样的环境


					Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问
					Ingress：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问
					Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer
					Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务

					Kubernetes中的应用将作为微服务运行，但是Kubernetes本身并没有给出微服务治理的解决方案，比如服务的限流、熔断、良好的灰度发布支持等。
					Traffic Management：API网关
					Observability：服务调用和性能分析
					Policy Enforcment：控制服务访问策略
					Service Identity and Security：安全保护

					Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关心服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。

					复杂环境下落地Service Mesh的挑战与实践:https://tech.meituan.com/2020/12/03/service-mesh-in-meituan.html

					OAM原则：其目标不仅限于 Kubernetes 之上的又一上层抽象，而是对于一切云服务，在基于资源对象的基础上，Trait 来控制 Kubernetes 中的一众高层次非可调度的资源对象，如 AutoScaler、Volume、Ingress，Istio 中的流量配置对象 VirtualService、DestinationRule 等，还可容纳更多的云服务，对于 Serverless 时代的去基础设施化的思想不谋而合，未来可期。

					CI/CD流程Kubernetes本身也没有提供
					Service Mesh：解决微服务治理问题
					Auto Pilot：自动驾驭能力，服务自动扩展，智能运维
				    FaaS/Serverless：用户无需再关注底层平台，只需要部署服务，根据服务的资源消耗和使用时间付费

					
					spring  vs k8s 整合 --- 如何融合
							1.最基础的服务注册发现功能来说，其方式分为客户端服务发现和服务端服务发现两种，Java应用中常用的方式是使用Eureka和Ribbon做服务注册发现和负载均衡，这属于客户端服务发现，而在Kubernetes中则可以使用DNS、Service和Ingress来实现，不需要修改应用代码，直接从网络层面来实现。
							2.流量路由规则（如负载均衡策略、入口路由、出口路由、百分比路由、限流、熔断、超时限制、故障注入等）、自动缩放策略、升级策略、发布策略等。


	 	k8s
	 			1.Kubernetes的目标旨在消除编排物理/虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。
	 			   Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。 Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。


	 			2.Kubernetes主要由以下几个核心组件组成：
					etcd保存了整个集群的状态；  == 
					apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； == gateway
					controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；  == cotrollerManager
					scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；
					kubelet负责维护容器的生命周期，同时也负责Volume（CSI）和网络（CNI）的管理；
					Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；
					kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；  === ? 如何查看
					除了核心组件，还有一些推荐的插件，其中有的已经成为CNCF中的托管项目：

					CoreDNS负责为整个集群提供DNS服务
					Ingress Controller为服务提供外网入口
					Prometheus提供资源监控
					Dashboard提供GUI
					Federation提供跨可用区的集群

				3.高层设计一定是从业务出发，而不是过早的从技术实现出发。
				  尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。
				  高低级功能 -- 降级 

				4.API 对象是 Kubernetes 集群中的管理操作单元。Kubernetes 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作。例如副本集 Replica Set 对应的 API 对象是 RS。

					每个 API 对象都有 3 大类属性：元数据 metadata、规范 spec 和状态 status。元数据是用来标识 API 对象的，每个对象都至少有 3 个元数据：namespace，name 和 uid；除此以外还有各种各样的标签 labels 用来标识和匹配不同的对象，例如用户可以用标签 env 来标识区分不同的服务部署环境，分别用 env=dev、env=testing、env=production 来标识开发、测试、生产的不同服务。规范描述了用户期望 Kubernetes 集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器 Replication Controller 设置期望的 Pod 副本数为 3；status 描述了系统实际当前达到的状态（Status），例如系统当前实际的 Pod 副本数为 2；那么复制控制器当前的程序逻辑就是自动启动新的 Pod，争取达到副本数为 3。

				pod:Pod 的设计理念是支持多个容器在一个 Pod 中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。
					目前 Kubernetes 中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为 Deployment、Job、DaemonSet 和 StatefulSet
				副本集（Replica Set，RS）
				部署（Deployment）:部署表示用户对 Kubernetes 集群的一次更新操作
				服务（Service）:RC、RS 和 Deployment 只是保证了支撑服务的微服务 Pod 的数量，但是没有解决如何访问这些服务的问题。一个 Pod 只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的 IP 启动一个新的 Pod，因此不能以确定的 IP 和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在 K8 集群中，客户端需要访问的服务就是 Service 对象。每个 Service 会对应一个集群内部有效的虚拟 IP，集群内部通过虚拟 IP 访问一个服务。在 Kubernetes 集群中微服务的负载均衡是由 Kube-proxy 实现的。Kube-proxy 是 Kubernetes 集群内部的负载均衡器。它是一个分布式代理服务器，在 Kubernetes 的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的 Kube-proxy 就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。
				任务（Job）
				后台支撑服务集（DaemonSet）:存储，日志和监控等在每个节点上支持 Kubernetes 集群运行的服务。
				有状态服务集（StatefulSet）:StatefulSet 是用来控制有状态服务，StatefulSet 中的每个 Pod 的名字都是事先确定的，不能更改。适合于 StatefulSet 的业务包括数据库服务 MySQL 和 PostgreSQL，集群化管理服务 ZooKeeper、etcd 等有状态服务.StatefulSet 做的只是将确定的 Pod 与确定的存储关联起来保证状态的连续性。
				集群联邦（Federation）:在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。Kubernetes 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 Kubernetes 的调度和计算存储连接要求。而联合集群服务就是为提供跨 Region 跨服务商 Kubernetes 集群服务而设计的。
				持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）:PV 和 Node 是资源的提供者，根据集群的基础设施变化而变化，由 Kubernetes 集群管理员配置；而 PVC 和 Pod 是资源的使用者，根据业务服务的需求变化而变化，有 Kubernetes 集群的使用者即服务的管理员来配置。

				节点（Node）:是所有 Pod 运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行 kubelet 管理节点上运行的容器。

				Etcd解析:用于保存集群所有的网络配置和对象的状态信息。

				Kubernetes作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：
				CRI（Container Runtime Interface）：容器运行时接口，提供计算资源
				CNI（Container Network Interface）：容器网络接口，提供网络资源
				CSI（Container Storage Interface）：容器存储接口，提供存储资源

				k8s中的网络：没有懂 ！！！
				Kubernetes本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。
				Node IP：宿主机的IP地址
				Pod IP：使用网络插件创建的IP（如flannel），使跨主机的Pod可以互通
				Cluster IP：虚拟IP，通过iptables规则访问服务


				controller-manager 控制pod 通过cofigmap
				命令：$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record
					controller-manager将这个文件放到sps上，searcher方到了数据库，本质都是文件
									  控制Pod的状态和生命周期。。如何控制的呢?
									  controller pod项目中就是连接rds，获取searcher配置信息，通过将
									  cotroller中application...信息塞入configmap来控制。其实applicaion中数据是从sps的values.yaml - application.property中控制的塞入到searcher的cofigmap中国 。。。。。
									  manger是来控制kv的。controller只有depliment. searcher有depliment和configmap

					删除pod
						1.为什么删除pod后会重新拉取新打包后的代码而不是...
						2.如何永久的删除一个pod

				init容器：
				pause容器：
				它们看到的网络设备、IP地址、Mac地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。这就是 Pod 解决网络共享的一个解法。
				Pod 安全策略 
					是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。


				存活（liveness）和就绪（readiness）探针

				pod hook
				Pod Preset

				node:为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建label和 taint 用于 pod 的调度等。  是物理机也可以是虚拟机

				kubectl get ns


				label vs group

				Taint 和 Toleration（污点和容忍）:高级调度功 .Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。

				操作： --- 为什么在通过db拉取时无法执行下面命令呢？ ******
					deployment：


					vcs-dataengine-searcher-history-damo-deployment
					kubectl scale deployment vcs-dataengine-searcher-history-damo-deployment --replicas 2


					kubectl get deployment vcs-dataengine-searcher-history-damo-deployment -o yaml


				3.7.1 == service
					1.微服务概念，servcie后面多个pod组成。ip不固定 === endpoint，但service名称是固定的 
					2.这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 ===========> vs springcloud 

						userspace 代理模式 == iptables 代理模式 == ipvs 代理模式	ipvs为负载均衡算法提供了更多选项，例如：  == Headless Service 应用仍然可以使用一种自注册的模式和适配器，对其它需要发现机制的系统能够很容易地基于这个 API 来构建。

																						rr：轮询调度
																						lc：最小连接数
																						dh：目标哈希
																						sh：源哈希
																						sed：最短期望延迟
																						nq： 不排队调度

					3.Kubernetes 支持2种基本的服务发现模式 —— 环境变量和 DNS。

					4.发布服务 —— 服务类型
						ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。
						NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 <NodeIP>:<NodePort>，可以从集群的外部访问一个 NodePort 服务。  这可以让开发人员自由地安装他们自己的负载均衡器

					5.Ingress 是从Kubernetes集群外部访问集群内部服务的入口	
						节点：Kubernetes集群中的一台物理机或者虚拟机。
						集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。
						边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。
						集群网络：一组逻辑或物理链接，可根据Kubernetes网络模型实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的OVS。
						服务：使用标签选择器标识一组pod成为的Kubernetes服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。


					
					1.Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种 Policy 等
					2.RBAC
					3.允许访问 kube-controller-manager 组件所需要的资源。

					1.为了管理存储，Kubernetes提供了Secret用于管理敏感信息，ConfigMap存储配置，Volume、PV、PVC、StorageClass等用来管理存储卷。
					2.ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
							设置环境变量的值
							在容器里设置命令行参数
							在数据卷里面创建config文件
					3.configmap热更新
							更新 ConfigMap 目前并不会触发相关 Pod 的滚动更新，可以通过修改 pod annotations 的方式强制触发滚动更新。
					4.Volume：容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。
						capacity 预期。 目前，存储大小是可以设置或请求的唯一资源。未来的属性可能包括 IOPS、吞吐量等。



					1.使用自定义资源扩展 API == 相当于接口  CRD（CustomResourceDefinition）
					
					1.Kubernetes作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能
					2.Kubernetes中有一个叫做kube-scheduler的组件，该组件就是专门监听kube-apiserver中是否有还未调度到node上的pod
					3.考虑以下两种情况：
						集群中有新增节点，想要让集群中的节点的资源利用率比较均衡一些，想要将一些高负载的节点上的pod驱逐到新增节点上，这是kuberentes的scheduler所不支持的，需要使用如descheduler这样的插件来实现。
						想要运行一些大数据应用，设计到资源分片，pod需要与数据分布达到一致均衡，避免个别节点处理大量数据，而其它节点闲置导致整个作业延迟，这时候可以考虑使用kube-batch。
					
					1.Kubelet使用liveness probe（存活探针） 资源不足或者bug，如果都不可用很有可能就是bug.否则就是资源问题
					2.Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。 Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。
					3.注意如果挂在的文件valume不可用，虽然起来了，但是进入pod后发现ps -ef|grep java没有该进程。虽然pod是running，但服务不可用。 ***
					  并且并没有从负载中移除

					1.通过抽象这些资源对象来控制k8s ResourceQuota -- 限额
					

				第五章： 真正实践：：：：：：==========未完成
					第一部分 在CentOS上部署kubernetes集群中介绍了如何通过二进制文件在CentOS物理机（也可以是公有云主机）上快速部署一个kubernetes集群。
					第二部分介绍如何在kubernetes中的服务发现与负载均衡。
					第三部分介绍如何运维kubernetes集群。
					第四部分介绍kubernetes中的存储管理。
					第五部分关于kubernetes集群和应用的监控。
					第六部分介绍kuberentes中的服务编排与管理。
					第七部分介绍如何基于kubernetes做持续集成与发布。
					第八部分是kubernetes集群与插件的更新升级。

				********************安装对应插件和elk.....
				0.在CentOS上部署kubernetes集群:https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html

				1.使用Vistio监控Istio服务网格中的流量
				2.链路追踪：opentracing 
				3.helm:
					1.Helm chart 是用来封装 Kubernetes 原生应用程序的 YAML 文件，可以在你部署应用的时候自定义应用程序的一些 metadata，便与应用程序的分发。
					2.Helm 和 chart 的主要作用是：
						应用程序封装
						版本管理
						依赖检查
						便于应用程序分发
					3.mychart  --- 我们还可以在 templates 目录加其他 Kubernetes 对象的配置，比如 ConfigMap、DaemonSet 等。
						├── Chart.yaml
						├── charts # 该目录保存其他依赖的 chart（子 chart）
						├── templates # chart 配置模板，用于渲染最终的 Kubernetes YAML 文件
						│   ├── NOTES.txt # 用户运行 helm install 时候的提示信息
						│   ├── _helpers.tpl # 用于创建模板时的帮助类
						│   ├── deployment.yaml # Kubernetes deployment 配置
						│   ├── ingress.yaml # Kubernetes ingress 配置
						│   ├── service.yaml # Kubernetes service 配置
						│   ├── serviceaccount.yaml # Kubernetes serviceaccount 配置
						│   └── tests
						│       └── test-connection.yaml
						└── values.yaml # 定义 chart 模板中的自定义配置的默认值，可以在执行 helm install 或 helm update 的时候覆盖

						Helm 常用命令如下：

							helm create：在本地创建新的 chart；
							helm dependency：管理 chart 依赖；
							helm intall：安装 chart；
							helm lint：检查 chart 配置是否有误；
							helm list：列出所有 release；
							helm package：打包本地 chart；
							helm repo：列出、增加、更新、删除 chart 仓库；
							helm rollback：回滚 release 到历史版本；
							helm pull：拉取远程 chart 到本地；
							helm search：使用关键词搜索 chart；
							helm uninstall：卸载 release；
							helm upgrade：升级 release；

						helm list|grep controller-manager  其实cotroller-manager和searcher不是两个应用而是一个。只是这个是管理节点。所以你打一个包，就是在一起，
						    在其中的helm install就是将包安装 

						Chart仓库


				4.jekins
					持续集成与发布，简称CI/CD，是微服务构建的重要环节，也是DevOps中推崇的方法论。

				5.dashboard:界面管理k8s...

					命令: docker --> k8s ---https://jimmysong.io/kubernetes-handbook/guide/kubectl-cheatsheet.html ****
						1.docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx
						   kubectl run --image=nginx nginx-app --port=80 --env="DOMAIN=cluster"
						2.docker ps
						  kubectl get po
						3.docker exec -ti a9ec34d98787 /bin/sh
						  kubectl exec -ti nginx-app-5jyvm -- /bin/sh 
						4.kubectl logs --previous nginx-app-zibvs
						5.kubectl get deployment nginx-app    kubectl delete deployment nginx-app
						6.kubectl top pod 

				  	1.每个Kubernetes集群都有一个集群根证书颁发机构（CA）。 集群中的组件通常使用CA来验证API server的证书，由API服务器验证kubelet客户端证书等。
				  	2.
					
					
					
					
					
					
					
					
				  	1.应用领域 -- service mesh --isotio --- 微服务解决方案 -- 网络模型
						1.Kubernetes 设计之初就是按照 Cloud Native 的理念设计的，Cloud Native 中有个重要概念就是微服务的架构设计，当将单体应用拆分微服务后， 随着服务数量的增多，如何微服务进行管理以保证服务的 SLA 呢？为了从架构层面上解决这个问题，解放程序员的创造性，避免繁琐的服务发现、监控、分布式追踪等事务，Service mesh 应运而生。
						2.微服务中的服务发现	
							在单体架构时，因为服务不会经常和动态迁移，所有服务地址可以直接在配置文件中配置，所以也不会有服务发现的问题。但是对于微服务来说，应用的拆分，服务之间的解耦，和服务动态扩展带来的服务迁移，服务发现就成了微服务中的一个关键问题。

						3.可观察性使用指标（应用/业务指标和运维指标。）、日志和追踪这些外部输出来理解系统的能力。这些指标、日志和追踪（Tracing）是基于系统内部的事件产生的。	
						4.什么是 service mesh？ == > https://jimmysong.io/kubernetes-handbook/usecases/service-mesh-fundamental.html
							可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。

							Service mesh 有如下几个特点： 

							应用程序间通讯的中间层
							轻量级网络代理
							应用程序无感知
							解耦应用程序的重试/超时、监控、追踪和服务发现

							service mesh分为:
								控制平面

								控制平面的特点：

								不直接解析数据包
								与控制平面中的代理通信，下发策略和配置
								负责网络行为的可视化
								通常提供API或者命令行工具可用于配置版本化管理，便于持续集成和部署
								数据平面

								数据平面的特点：

								通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的
								直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等
								对应用来说透明，即可以做到无感知部署

						5.在 Cloud Native 架构下，容器的使用给予了异构应用程序的更多可行性，kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。===> serive mesh 分布式限流.重试...

						6.为什么有了如Kubernetes这样的容器编排我们还需要Service Mesh呢，下表是对容器编排调度器的核心功能和缺少的服务级别能力对比。

											核心能力				缺少的服务级别能力

											集群管理					熔断
											调度						L7细粒度的流量控制
											编排器和主机维护			混沌测试
											服务发现					金丝雀部署
											网络和负载均衡			超时、重试、 budget和deadline
											有状态服务				按请求路由
											多租户、多region			策略
											简单的应用监控检查和性能监控	传输层安全（加密）
											应用部署					身份和访问控制
											配置和秘钥管理			配额管理
											/						协议转换（REST、gRPC）


											Service Mesh还有一些遗留的问题没有解决或者说比较薄弱的功能：

									分布式应用的调试，可以参考squash
									服务拓扑和状态图，可以参考kiali和vistio
									多租户和多集群的支持
									白盒监控、支持APM
									加强负载测试工具slow_cooker、fortio、lago等
									更高级的fallback路径支持
									可拔插的证书授权组建，支持外部的CA
									下面是采纳Service Mesh之前需要考虑的因素。

									因素	可以考虑使用Service Mesh        				强烈建议使用Service Mesh
									服务通信	基本无需跨服务间的通讯						十分要求服务间通讯
									可观察性	只关注边缘的指标即可							内部服务和边缘指标都要考虑以更好的了解服务的行为
									客户关注	主要关注外部API的体验，						内外用户是隔离的	内部外部用户没有区别体验一致
									API的界限											API主要是作为客户端为客户提供，内部的API与外部是分离的	API即产品，API就是你的产品能力
									安全模型	通过边缘、防火墙可信内部网络的方式控制安全	    所有的服务都需要认证和鉴权、服务间要加密、zero-trust安全观念

							7.istio:https://jimmysong.io/kubernetes-handbook/usecases/istio.html
							Istio 解决了开发人员和运维人员所面临的从单体应用向分布式微服务架构转变的挑战。了解它是如何做到这一点的可以让我们更详细地理解 Istio 的服务网格。
							它的需求包括服务发现、负载均衡、故障恢复、度量和监控等。服务网格通常还有更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。


							8. springcloud 和 k8s实现微服务的对比以及整合 k8s只是解决了容器编排和部分微服务==》 基于容器在网络基础上service mesh中isotio解决微服务解决方案  

							9.从云---自来水管到serviceless :--以app为最小单位
									就像无线互联网实际有的地方也需要用到有线连接一样，无服务器架构仍然在某处有服务器。Serverless（无服务器架构）指的是由开发者实现的服务端逻辑运行在无状态的计算容器中，它由事件触发， 完全被第三方管理，其业务层面的状态则被开发者使用的数据库和存储资源所记录。

							10.iaas -- paas --  saas  --- baas(backend ..) -- faas(function)=serverless


				  		

				  	1.本地环境搭建

				  	1.基金会/博客/版本迭代/线下交流....
